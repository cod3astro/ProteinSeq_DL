{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ad29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial - Memory Usage: 0.28 GB\n",
      "\n",
      "STARTING PROTEIN GO TERM PROCESSING\n",
      "Start Time: 11:24:53\n",
      "Initial Memory: 0.28 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "def print_memory_usage(step_name):\n",
    "    mem_info = process.memory_info()\n",
    "    mem_gb = mem_info.rss / (1024 ** 3)\n",
    "\n",
    "    print(f\"{step_name} - Memory Usage: {mem_gb:.2f} GB\")\n",
    "    return mem_gb\n",
    "\n",
    "def print_step_header(step_num, step_name):\n",
    "    separator = \"=\" * 60\n",
    "\n",
    "    print(\"\\n\" + separator)\n",
    "    print(f\"STEP {step_num}: {step_name}\")\n",
    "    print(separator)\n",
    "\n",
    "initial_mem = print_memory_usage(\"Initial\")\n",
    "\n",
    "print(\"\\nSTARTING PROTEIN GO TERM PROCESSING\")\n",
    "print(f\"Start Time: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Initial Memory: {initial_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eab1790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV entries: 105951\n",
      "FASTA sequences: 105951\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "tsv_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/raw/train/uniprotkb_AND_reviewed_true_AND_protein_2025_12_27.tsv\"\n",
    "labels_df = pd.read_csv(tsv_path, sep='\\t')\n",
    "\n",
    "fasta_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/raw/train/uniprotkb_AND_reviewed_true_AND_protein_2025_12_27.fasta\"\n",
    "sequence_dict = {}\n",
    "\n",
    "for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "\n",
    "    header = record.id\n",
    "    if \"|\" in header:\n",
    "        protein_id = header.split(\"|\")[1]\n",
    "    else:\n",
    "        protein_id = header.split()[0]\n",
    "    sequence_dict[protein_id] = str(record.seq)\n",
    "\n",
    "print(f\"TSV entries: {len(labels_df)}\")\n",
    "print(f\"FASTA sequences: {len(sequence_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bfaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_go_terms(go_string):\n",
    "\n",
    "    if pd.isna(go_string) or go_string == \"\":\n",
    "        return []\n",
    "\n",
    "    return [term.strip() for term in str(go_string).split(';')]\n",
    "\n",
    "labels_df['go_terms_list'] = labels_df['Gene Ontology IDs'].apply(parse_go_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9857fc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology IDs</th>\n",
       "      <th>go_terms_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>ABTIR_ACIB9</td>\n",
       "      <td>2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...</td>\n",
       "      <td>Acinetobacter baumannii (strain 1295743)</td>\n",
       "      <td>MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...</td>\n",
       "      <td>GO:0003953; GO:0007165; GO:0019677; GO:0050135...</td>\n",
       "      <td>[GO:0003953, GO:0007165, GO:0019677, GO:005013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>ENG1_RHIMI</td>\n",
       "      <td>Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...</td>\n",
       "      <td>Rhizomucor miehei</td>\n",
       "      <td>MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...</td>\n",
       "      <td>GO:0000272; GO:0005576; GO:0042973; GO:0052861...</td>\n",
       "      <td>[GO:0000272, GO:0005576, GO:0042973, GO:005286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024B7W1</td>\n",
       "      <td>POLG_ZIKVF</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Zika virus (isolate ZIKV/Human/French Polynesi...</td>\n",
       "      <td>MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...</td>\n",
       "      <td>GO:0003724; GO:0003725; GO:0003968; GO:0004252...</td>\n",
       "      <td>[GO:0003724, GO:0003725, GO:0003968, GO:000425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A024RXP8</td>\n",
       "      <td>GUX1_HYPJR</td>\n",
       "      <td>Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "      <td>MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...</td>\n",
       "      <td>GO:0005576; GO:0016162; GO:0030245; GO:0030248</td>\n",
       "      <td>[GO:0005576, GO:0016162, GO:0030245, GO:0030248]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>CUTI1_HYPJR</td>\n",
       "      <td>Cutinase (EC 3.1.1.74)</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "      <td>MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...</td>\n",
       "      <td>GO:0005576; GO:0016052; GO:0050525</td>\n",
       "      <td>[GO:0005576, GO:0016052, GO:0050525]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry   Entry Name                                      Protein names  \\\n",
       "0  A0A009IHW8  ABTIR_ACIB9  2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...   \n",
       "1  A0A023I7E1   ENG1_RHIMI  Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...   \n",
       "2  A0A024B7W1   POLG_ZIKVF  Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "3  A0A024RXP8   GUX1_HYPJR  Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...   \n",
       "4  A0A024SC78  CUTI1_HYPJR                             Cutinase (EC 3.1.1.74)   \n",
       "\n",
       "                                            Organism  \\\n",
       "0           Acinetobacter baumannii (strain 1295743)   \n",
       "1                                  Rhizomucor miehei   \n",
       "2  Zika virus (isolate ZIKV/Human/French Polynesi...   \n",
       "3  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...   \n",
       "4  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...   \n",
       "\n",
       "                                            Sequence  \\\n",
       "0  MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
       "1  MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
       "2  MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...   \n",
       "3  MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...   \n",
       "4  MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...   \n",
       "\n",
       "                                   Gene Ontology IDs  \\\n",
       "0  GO:0003953; GO:0007165; GO:0019677; GO:0050135...   \n",
       "1  GO:0000272; GO:0005576; GO:0042973; GO:0052861...   \n",
       "2  GO:0003724; GO:0003725; GO:0003968; GO:0004252...   \n",
       "3     GO:0005576; GO:0016162; GO:0030245; GO:0030248   \n",
       "4                 GO:0005576; GO:0016052; GO:0050525   \n",
       "\n",
       "                                       go_terms_list  \n",
       "0  [GO:0003953, GO:0007165, GO:0019677, GO:005013...  \n",
       "1  [GO:0000272, GO:0005576, GO:0042973, GO:005286...  \n",
       "2  [GO:0003724, GO:0003725, GO:0003968, GO:000425...  \n",
       "3   [GO:0005576, GO:0016162, GO:0030245, GO:0030248]  \n",
       "4               [GO:0005576, GO:0016052, GO:0050525]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51798798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched proteins: 105951\n",
      "Saved to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/training_data_combined.csv\n"
     ]
    }
   ],
   "source": [
    "filtered_df = labels_df[labels_df['Entry'].isin(sequence_dict.keys())].copy()\n",
    "\n",
    "filtered_df['sequence'] = filtered_df['Entry'].map(sequence_dict)\n",
    "\n",
    "train_df = filtered_df[['Entry', 'sequence', 'go_terms_list', 'Organism']].rename(\n",
    "    columns={\n",
    "        'Entry': 'accession',\n",
    "        'go_terms_list': 'go_terms',\n",
    "        'Organism': 'organism'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Matched proteins: {len(train_df)}\")\n",
    "\n",
    "output_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/training_data_combined.csv\"\n",
    "train_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82c79f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>sequence</th>\n",
       "      <th>go_terms</th>\n",
       "      <th>organism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...</td>\n",
       "      <td>[GO:0003953, GO:0007165, GO:0019677, GO:005013...</td>\n",
       "      <td>Acinetobacter baumannii (strain 1295743)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...</td>\n",
       "      <td>[GO:0000272, GO:0005576, GO:0042973, GO:005286...</td>\n",
       "      <td>Rhizomucor miehei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024B7W1</td>\n",
       "      <td>MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...</td>\n",
       "      <td>[GO:0003724, GO:0003725, GO:0003968, GO:000425...</td>\n",
       "      <td>Zika virus (isolate ZIKV/Human/French Polynesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A024RXP8</td>\n",
       "      <td>MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...</td>\n",
       "      <td>[GO:0005576, GO:0016162, GO:0030245, GO:0030248]</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...</td>\n",
       "      <td>[GO:0005576, GO:0016052, GO:0050525]</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accession                                           sequence  \\\n",
       "0  A0A009IHW8  MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
       "1  A0A023I7E1  MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
       "2  A0A024B7W1  MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...   \n",
       "3  A0A024RXP8  MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...   \n",
       "4  A0A024SC78  MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...   \n",
       "\n",
       "                                            go_terms  \\\n",
       "0  [GO:0003953, GO:0007165, GO:0019677, GO:005013...   \n",
       "1  [GO:0000272, GO:0005576, GO:0042973, GO:005286...   \n",
       "2  [GO:0003724, GO:0003725, GO:0003968, GO:000425...   \n",
       "3   [GO:0005576, GO:0016162, GO:0030245, GO:0030248]   \n",
       "4               [GO:0005576, GO:0016052, GO:0050525]   \n",
       "\n",
       "                                            organism  \n",
       "0           Acinetobacter baumannii (strain 1295743)  \n",
       "1                                  Rhizomucor miehei  \n",
       "2  Zika virus (isolate ZIKV/Human/French Polynesi...  \n",
       "3  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...  \n",
       "4  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade13802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins: 105951\n",
      "\n",
      "Collecting unique GO terms and computing frequencies...\n",
      "Found 27615 unique GO terms total\n",
      "After collecting all terms - Memory Usage: 0.54 GB\n",
      "\n",
      "Selected 2000 frequent + 761 random = 2761 total GO terms\n",
      "Most common term: GO:0005737 (appears 22662 times)\n",
      "Sample random term: GO:0016311 (appears 62 times)\n",
      "After term selection - Memory Usage: 0.54 GB\n",
      "Time elapsed: 8.7 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total proteins: {len(train_df)}\")\n",
    "\n",
    "print(\"\\nCollecting unique GO terms and computing frequencies...\")\n",
    "go_counts = Counter(term for go_list in train_df['go_terms'] for term in go_list)\n",
    "all_go_terms = set(go_counts.keys())\n",
    "\n",
    "print(f\"Found {len(all_go_terms)} unique GO terms total\")\n",
    "mem1 = print_memory_usage(\"After collecting all terms\")\n",
    "\n",
    "ultra_general = {'GO:0008150', 'GO:0005575', 'GO:0003674'}\n",
    "\n",
    "filtered_terms = [\n",
    "    term for term, count in go_counts.most_common()\n",
    "    if term not in ultra_general\n",
    "]\n",
    "\n",
    "top_n_frequent = 2000\n",
    "top_go_frequent = filtered_terms[:top_n_frequent]\n",
    "\n",
    "remaining_terms = filtered_terms[top_n_frequent:]\n",
    "\n",
    "remaining_filtered = [\n",
    "    term for term in remaining_terms\n",
    "    if go_counts[term] >= 5\n",
    "]\n",
    "np.random.seed(42)\n",
    "\n",
    "top_go_random = (\n",
    "    np.random.choice(remaining_filtered, 761, replace=False).tolist()\n",
    "    if len(remaining_filtered) >= 761\n",
    "    else remaining_filtered\n",
    ")\n",
    "\n",
    "top_go_terms = top_go_frequent + top_go_random\n",
    "\n",
    "print(f\"\\nSelected {len(top_go_frequent)} frequent + \"\n",
    "      f\"{len(top_go_random)} random = {len(top_go_terms)} total GO terms\")\n",
    "\n",
    "print(f\"Most common term: {top_go_frequent[0]} \"\n",
    "      f\"(appears {go_counts[top_go_frequent[0]]} times)\")\n",
    "\n",
    "if top_go_random:\n",
    "    print(f\"Sample random term: {top_go_random[0]} \"\n",
    "          f\"(appears {go_counts[top_go_random[0]]} times)\")\n",
    "\n",
    "mem2 = print_memory_usage(\"After term selection\")\n",
    "\n",
    "print(f\"Time elapsed: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609f21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins: 105,951\n",
      "Total selected GO terms: 2,761\n",
      "Created binary DataFrame with 2,761 label columns\n",
      "After binary matrix creation - Memory Usage: 0.50 GB\n",
      "Matrix filling time: 2.4 seconds\n",
      "Total time elapsed: 11.1 seconds\n"
     ]
    }
   ],
   "source": [
    "go_to_index = {go: idx for idx, go in enumerate(top_go_terms)}\n",
    "\n",
    "num_proteins = len(train_df)\n",
    "num_labels = len(top_go_terms)\n",
    "\n",
    "print(f\"Total proteins: {num_proteins:,}\")\n",
    "print(f\"Total selected GO terms: {num_labels:,}\")\n",
    "\n",
    "binary_matrix = np.zeros((num_proteins, num_labels), dtype=np.int8)\n",
    "fill_start_time = time.time()\n",
    "for protein_idx, go_list in enumerate(train_df['go_terms']):\n",
    "    for go in go_list:\n",
    "        if go in go_to_index:\n",
    "            label_idx = go_to_index[go]\n",
    "\n",
    "            binary_matrix[protein_idx, label_idx] = 1\n",
    "column_names = [f\"label_{go}\" for go in top_go_terms]\n",
    "\n",
    "binary_df = pd.DataFrame(binary_matrix, columns=column_names)\n",
    "\n",
    "print(f\"Created binary DataFrame with {binary_df.shape[1]:,} label columns\")\n",
    "\n",
    "del binary_matrix\n",
    "gc.collect()\n",
    "mem3 = print_memory_usage(\"After binary matrix creation\")\n",
    "\n",
    "print(f\"Matrix filling time: {time.time() - fill_start_time:.1f} seconds\")\n",
    "print(f\"Total time elapsed: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303affe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: 105,951 proteins √ó 2,763 columns\n",
      "\n",
      "VERIFICATION:\n",
      "Example protein: A0A009IHW8\n",
      "Sequence length: 269\n",
      "Number of GO term labels: 2,761\n",
      "Positive labels for first protein: 2 out of 2761\n",
      "All labels have at least one positive example\n",
      "After final dataframe - Memory Usage: 0.57 GB\n",
      "Total time elapsed: 16.6 seconds\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat(\n",
    "    [\n",
    "        train_df[['sequence', 'accession']],  # original features\n",
    "        binary_df                              # binary label matrix\n",
    "    ], axis=1\n",
    ")\n",
    "print(f\"Final DataFrame shape: {final_df.shape[0]:,} proteins √ó {final_df.shape[1]:,} columns\")\n",
    "\n",
    "del binary_df\n",
    "gc.collect()\n",
    "print(\"\\nVERIFICATION:\")\n",
    "\n",
    "example_accession = final_df.iloc[0]['accession']\n",
    "example_sequence = final_df.iloc[0]['sequence']\n",
    "\n",
    "print(f\"Example protein: {example_accession}\")\n",
    "print(f\"Sequence length: {len(example_sequence):,}\")\n",
    "\n",
    "label_columns = [col for col in final_df.columns if col.startswith('label_')]\n",
    "print(f\"Number of GO term labels: {len(label_columns):,}\")\n",
    "\n",
    "positive_counts = final_df.loc[0, label_columns].sum()\n",
    "print(f\"Positive labels for first protein: {positive_counts} out of {len(label_columns)}\")\n",
    "empty_labels = [col for col in label_columns if final_df[col].sum() == 0]\n",
    "\n",
    "if empty_labels:\n",
    "    print(f\"Warning: {len(empty_labels)} labels have no positive examples!\")\n",
    "else:\n",
    "    print(\"All labels have at least one positive example\")\n",
    "\n",
    "mem4 = print_memory_usage(\"After final dataframe\")\n",
    "print(f\"Total time elapsed: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209c6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 105,951 proteins\n",
      "Number of label columns: 2,761\n",
      "Conversion time: 24.9 seconds\n",
      "Saving HuggingFace Dataset to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105951/105951 [23:51<00:00, 73.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n",
      "Saved metadata to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/go_terms_metadata.csv\n",
      "Final - Memory Usage: 0.59 GB\n",
      "\n",
      "PERFORMANCE SUMMARY:\n",
      "Total processing time: 1473.7 seconds (24.6 minutes)\n",
      "Memory increase: 0.30 GB\n",
      "Time: 11:49:27\n",
      "\n",
      "DATA SUMMARY:\n",
      "  ‚Ä¢ Input proteins: 105,951\n",
      "  ‚Ä¢ Unique GO terms (original): 27,615\n",
      "  ‚Ä¢ Selected GO terms: 2,761\n",
      "  ‚Ä¢ Final dataset size: 105,951 √ó 2,763\n",
      "  ‚Ä¢ Label representation: multi-hot binary (int8)\n",
      "\n",
      "All steps completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hf_start_time = time.time()\n",
    "\n",
    "dataset = Dataset.from_pandas(final_df, preserve_index=False)\n",
    "print(f\"Dataset created with {len(dataset):,} proteins\")\n",
    "\n",
    "label_columns = [col for col in dataset.column_names if col.startswith('label_')]\n",
    "print(f\"Number of label columns: {len(label_columns):,}\")\n",
    "\n",
    "print(f\"Conversion time: {time.time() - hf_start_time:.1f} seconds\")\n",
    "\n",
    "save_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "print(f\"Saving HuggingFace Dataset to: {save_path}\")\n",
    "\n",
    "dataset.save_to_disk(save_path)\n",
    "print(\"Dataset saved successfully.\")\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    'go_term': top_go_terms,\n",
    "    'count': [go_counts[term] for term in top_go_terms],\n",
    "    'percentage': [(go_counts[term] / len(final_df)) * 100 for term in top_go_terms],\n",
    "    'is_frequent': [True] * len(top_go_frequent) + [False] * len(top_go_random)\n",
    "})\n",
    "\n",
    "metadata_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/go_terms_metadata.csv\"\n",
    "metadata.to_csv(metadata_path, index=False)\n",
    "print(f\"Saved metadata to: {metadata_path}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "final_mem = print_memory_usage(\"Final\")\n",
    "\n",
    "print(\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(f\"Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Memory increase: {final_mem - initial_mem:.2f} GB\")\n",
    "print(f\"Time: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nDATA SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Input proteins: {len(train_df):,}\")\n",
    "print(f\"  ‚Ä¢ Unique GO terms (original): {len(all_go_terms):,}\")\n",
    "print(f\"  ‚Ä¢ Selected GO terms: {len(top_go_terms):,}\")\n",
    "print(f\"  ‚Ä¢ Final dataset size: {final_df.shape[0]:,} √ó {final_df.shape[1]:,}\")\n",
    "print(f\"  ‚Ä¢ Label representation: multi-hot binary (int8)\")\n",
    "\n",
    "print(\"\\nAll steps completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9cc1cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "After imports - Memory Usage: 0.63 GB\n",
      "‚úì Random seed set to: 42\n",
      "After setting seeds - Memory Usage: 0.63 GB\n",
      "Loading dataset...\n",
      "Before loading - Memory Usage: 0.63 GB\n",
      "‚úì Loaded: 105,951 proteins\n",
      "After loading - Memory Usage: 0.73 GB\n",
      "‚úì Memory used: 0.107 GB\n",
      "‚úì Number of GO term labels: 2,761\n",
      "‚úì Dataset formatted for PyTorch (labels as tensors)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print_memory_usage(\"After imports\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    print(f\"‚úì Random seed set to: {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "print_memory_usage(\"After setting seeds\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "mem_before = print_memory_usage(\"Before loading\")\n",
    "dataset_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"‚úì Loaded: {len(dataset):,} proteins\")\n",
    "\n",
    "mem_after = print_memory_usage(\"After loading\")\n",
    "print(f\"‚úì Memory used: {mem_after - mem_before:.3f} GB\")\n",
    "\n",
    "label_columns = [col for col in dataset.column_names if col.startswith(\"label_\")]\n",
    "print(f\"‚úì Number of GO term labels: {len(label_columns):,}\")\n",
    "\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=label_columns  # labels as tensors\n",
    ")\n",
    "print(\"‚úì Dataset formatted for PyTorch (labels as tensors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bd380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset loaded\n",
      "Total proteins: 105951\n",
      "\n",
      "üìä Sequence Length Statistics\n",
      "Min length: 2\n",
      "Max length: 35213\n",
      "Mean length: 472.26\n",
      "Median length: 358.0\n",
      "Std deviation: 535.95\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(\"‚úì Dataset loaded\")\n",
    "print(f\"Total proteins: {len(dataset)}\")\n",
    "\n",
    "seq_lengths = [len(seq) for seq in dataset[\"sequence\"]]\n",
    "seq_lengths = np.array(seq_lengths)\n",
    "\n",
    "print(\"\\nüìä Sequence Length Statistics\")\n",
    "print(f\"Min length: {seq_lengths.min()}\")\n",
    "print(f\"Max length: {seq_lengths.max()}\")\n",
    "print(f\"Mean length: {seq_lengths.mean():.2f}\")\n",
    "print(f\"Median length: {np.median(seq_lengths)}\")\n",
    "print(f\"Std deviation: {seq_lengths.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2da4f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Sequence Length Distribution\n",
      "   0 - 250  : 35261 proteins\n",
      " 250 - 500  : 37285 proteins\n",
      " 500 - 750  : 17379 proteins\n",
      " 750 - 1000 : 7145 proteins\n",
      "1000 - 1250 : 3509 proteins\n",
      "1250 - 1500 : 1878 proteins\n",
      "1500 - 1750 : 956 proteins\n",
      "1750 - ‚àû    : 2538 proteins\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 2000, 250))  # 0,250,500,...,2000\n",
    "bins.append(float(\"inf\"))         # >2000 bin\n",
    "\n",
    "hist, bin_edges = np.histogram(seq_lengths, bins=bins)\n",
    "\n",
    "print(\"\\nüì¶ Sequence Length Distribution\")\n",
    "for i in range(len(hist)):\n",
    "    start = int(bin_edges[i])\n",
    "    end = int(bin_edges[i+1]) if bin_edges[i+1] != float(\"inf\") else \"‚àû\"\n",
    "    print(f\"{start:>4} - {end:<4} : {hist[i]} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ec6de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking for duplicate sequences...\n",
      "Total sequences: 105951\n",
      "Unique sequences: 102750\n",
      "Duplicate sequences: 3201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Checking for duplicate sequences...\")\n",
    "\n",
    "unique_sequences = set(dataset[\"sequence\"]) \n",
    "total_sequences = len(dataset) \n",
    "unique_count = len(unique_sequences) \n",
    "duplicates = total_sequences - unique_count\n",
    "\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "print(f\"Unique sequences: {unique_count}\")\n",
    "print(f\"Duplicate sequences: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed25482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Removing duplicate sequences (Arrow-safe method)...\n",
      "Original dataset size: 105951\n",
      "Keeping 102750 unique sequences\n",
      "New dataset size: 102750\n",
      "Removed duplicates: 3201\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Removing duplicate sequences (Arrow-safe method)...\")\n",
    "\n",
    "original_size = len(dataset)\n",
    "print(f\"Original dataset size: {original_size}\")\n",
    "\n",
    "sequences = dataset[\"sequence\"]\n",
    "\n",
    "seen = {}\n",
    "keep_indices = []\n",
    "for idx, seq in enumerate(sequences):\n",
    "    if seq not in seen:\n",
    "        seen[seq] = True\n",
    "        keep_indices.append(idx)\n",
    "\n",
    "print(f\"Keeping {len(keep_indices)} unique sequences\")\n",
    "dataset = dataset.select(keep_indices)\n",
    "\n",
    "new_size = len(dataset)\n",
    "print(f\"New dataset size: {new_size}\")\n",
    "print(f\"Removed duplicates: {original_size - new_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89fde5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ProteinDataset class created successfully\n",
      "After creating dataset class - Memory Usage: 0.82 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8194694519042969"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "\n",
    "    def __init__(self, hf_dataset, max_seq_length=512):\n",
    "        self.dataset = hf_dataset\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.label_cols = [\n",
    "            col for col in hf_dataset.column_names\n",
    "            if col.startswith(\"label_\")\n",
    "        ]\n",
    "\n",
    "        self.amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        self.aa_to_idx = {\n",
    "            aa: i + 1 for i, aa in enumerate(self.amino_acids)\n",
    "        }\n",
    "        print(f\"  Found {len(self.label_cols)} GO term labels\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataset[idx][\"sequence\"]\n",
    "        encoded_seq = self.encode_sequence(sequence)\n",
    "\n",
    "        labels = [\n",
    "            self.dataset[idx][col]\n",
    "            for col in self.label_cols\n",
    "        ]\n",
    "        return {\n",
    "            \"sequence\": torch.tensor(encoded_seq, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def encode_sequence(self, sequence):\n",
    "        sequence = sequence[:self.max_seq_length]\n",
    "        encoded = [\n",
    "            self.aa_to_idx.get(aa, 0)  # 0 if unknown\n",
    "            for aa in sequence\n",
    "        ]\n",
    "\n",
    "        padding_length = self.max_seq_length - len(encoded)\n",
    "        if padding_length > 0:\n",
    "            encoded += [0] * padding_length\n",
    "\n",
    "        return encoded\n",
    "\n",
    "print(\"‚úì ProteinDataset class created successfully\")\n",
    "print_memory_usage(\"After creating dataset class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10c0ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before splitting data - Memory Usage: 0.82 GB\n",
      "  Found 2761 GO term labels\n",
      "  Found 2761 GO term labels\n",
      "  Found 2761 GO term labels\n",
      "‚úì Data split completed:\n",
      "  Training samples: 71,925\n",
      "  Validation samples: 15,412\n",
      "  Test samples: 15,413\n",
      "After splitting data - Memory Usage: 0.84 GB\n",
      "  Memory used by splits: 0.02 GB\n",
      "  Garbage collection performed\n",
      "After garbage collection - Memory Usage: 0.84 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.839019775390625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_before = print_memory_usage(\"Before splitting data\")\n",
    "\n",
    "split_1 = dataset.train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset_arrow = split_1[\"train\"]\n",
    "temp_dataset_arrow = split_1[\"test\"]\n",
    "\n",
    "split_2 = temp_dataset_arrow.train_test_split(\n",
    "    test_size=0.5,\n",
    "    seed=42\n",
    ")\n",
    "val_dataset_arrow = split_2[\"train\"]\n",
    "test_dataset_arrow = split_2[\"test\"]\n",
    "\n",
    "train_dataset = ProteinDataset(train_dataset_arrow, max_seq_length=512)\n",
    "val_dataset = ProteinDataset(val_dataset_arrow, max_seq_length=512)\n",
    "test_dataset = ProteinDataset(test_dataset_arrow, max_seq_length=512)\n",
    "\n",
    "print(\"‚úì Data split completed:\")\n",
    "print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "print(f\"  Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"  Test samples: {len(test_dataset):,}\")\n",
    "\n",
    "mem_after = print_memory_usage(\"After splitting data\")\n",
    "print(f\"  Memory used by splits: {mem_after - mem_before:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"  Garbage collection performed\")\n",
    "print_memory_usage(\"After garbage collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d596d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SimpleProteinCNN class created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model parameters: 273,385\n",
      "After creating model - Memory Usage: 0.84 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8419723510742188"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleProteinCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=21,\n",
    "            embedding_dim=64,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(128, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úì SimpleProteinCNN class created\")\n",
    "\n",
    "sample_model = SimpleProteinCNN(num_classes=len(train_dataset.label_cols))\n",
    "total_params = sum(p.numel() for p in sample_model.parameters())\n",
    "\n",
    "print(f\"  Model parameters: {total_params:,}\")\n",
    "print_memory_usage(\"After creating model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478941d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 14: PREPARING FOR TRAINING - SIMPLE VERSION\n",
      "============================================================\n",
      "Step 1: Setting up device...\n",
      "‚úì Using CPU (most reliable for laptops)\n",
      "\n",
      "Step 2: Checking our data...\n",
      "We have 71925 training proteins\n",
      "Found 2761 GO term labels\n",
      "\n",
      "Step 3: Creating data loaders...\n",
      "‚úì Created data loaders\n",
      "  Batch size: 8\n",
      "  Training batches: 8991\n",
      "\n",
      "Creating a simpler model...\n",
      "‚úì Created VerySimpleProteinCNN\n",
      "  Parameters: 13,380\n",
      "\n",
      "Step 5: Setting up optimizer (with workaround)...\n",
      "‚úì Created loss function\n",
      "Trying Adam optimizer...\n",
      "‚úì Adam optimizer created successfully!\n",
      "\n",
      "Step 6: Testing everything works...\n",
      "‚úì Got batch: sequences=torch.Size([512]), labels=torch.Size([512, 8])\n",
      "‚ùå Test failed: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3\n",
      "\n",
      "Debugging info:\n",
      "  Device: cpu\n",
      "  Model type: <class '__main__.VerySimpleProteinCNN'>\n",
      "  Model on device? cpu\n",
      "\n",
      "Trying to fix...\n",
      "‚úì Reset model and optimizer\n",
      "\n",
      "==================================================\n",
      "FINAL TRAINING SETUP:\n",
      "==================================================\n",
      "1. Device: cpu\n",
      "2. Model: VerySimpleProteinCNN\n",
      "3. Parameters: 13,380\n",
      "4. Optimizer: SGD\n",
      "5. Batch size: 8\n",
      "6. Training samples: 71925\n",
      "7. Validation samples: 15412\n",
      "After setup - Memory Usage: 0.93 GB\n",
      "\n",
      "==================================================\n",
      "READY FOR TRAINING!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Setting up device...\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(\"‚úì Using CPU (most reliable for laptops)\")\n",
    "print(\"\\nStep 2: Checking our data...\")\n",
    "print(f\"We have {len(train_dataset)} training proteins\")\n",
    "\n",
    "if hasattr(train_dataset, 'label_cols'):\n",
    "    print(f\"Found {len(train_dataset.label_cols)} GO term labels\")\n",
    "else:\n",
    "    print(\"Warning: No label columns found!\")\n",
    "    num_dummy_labels = 100\n",
    "    print(f\"Creating {num_dummy_labels} dummy labels for testing\")\n",
    "\n",
    "print(\"\\nStep 3: Creating data loaders...\")\n",
    "\n",
    "batch_size = 8 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0  \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created data loaders\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(\"\\nCreating a simpler model...\")\n",
    "\n",
    "class VerySimpleProteinCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=100):\n",
    "        super(VerySimpleProteinCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(21, 32, padding_idx=0)\n",
    "        \n",
    "        self.conv = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.relu(self.conv(x))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "try:\n",
    "    num_classes = 100  \n",
    "    model = VerySimpleProteinCNN(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úì Created VerySimpleProteinCNN\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {total_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    print(\"Creating the simplest possible model...\")\n",
    "    \n",
    "    class LinearModel(nn.Module):\n",
    "        def __init__(self, num_classes=100):\n",
    "            super(LinearModel, self).__init__()\n",
    "            self.fc = nn.Linear(256, num_classes)  # Sequence length is 256\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.fc(x.float().mean(dim=1))\n",
    "    \n",
    "    model = LinearModel(num_classes=100)\n",
    "    model = model.to(device)\n",
    "    print(\"‚úì Created LinearModel (simplest possible)\")\n",
    "\n",
    "print(\"\\nStep 5: Setting up optimizer (with workaround)...\")\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(f\"‚úì Created loss function\")\n",
    "\n",
    "optimizers_to_try = [\n",
    "    ('SGD', optim.SGD(model.parameters(), lr=0.01)),\n",
    "    ('RMSprop', optim.RMSprop(model.parameters(), lr=0.001)),\n",
    "    ('Adagrad', optim.Adagrad(model.parameters(), lr=0.01))\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(\"Trying Adam optimizer...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "    print(\"‚úì Adam optimizer created successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Adam failed: {e}\")\n",
    "    print(\"Trying SGD instead...\")\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    print(\"‚úì Using SGD optimizer instead\")\n",
    "\n",
    "print(\"\\nStep 6: Testing everything works...\")\n",
    "\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    sequences = test_batch['sequence'].to(device)\n",
    "    labels = test_batch['labels'].to(device)\n",
    "    \n",
    "    print(f\"‚úì Got batch: sequences={sequences.shape}, labels={labels.shape}\")\n",
    "    \n",
    "    with torch.no_grad():  # Don't calculate gradients for testing\n",
    "        outputs = model(sequences)\n",
    "        print(f\"‚úì Model can make predictions: outputs={outputs.shape}\")\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    print(f\"‚úì Can calculate loss: {loss.item():.4f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"‚úì Can do training step (backpropagation)\")\n",
    "    \n",
    "    print(\"\\n ALL TESTS PASSED! Ready for training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    print(\"\\nDebugging info:\")\n",
    "    \n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Model type: {type(model)}\")\n",
    "    print(f\"  Model on device? {next(model.parameters()).device}\")\n",
    "    \n",
    "    print(\"\\nTrying to fix...\")\n",
    "    \n",
    "    model = VerySimpleProteinCNN(num_classes=100)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    print(\"‚úì Reset model and optimizer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TRAINING SETUP:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"1. Device: {device}\")\n",
    "print(f\"2. Model: {model.__class__.__name__}\")\n",
    "print(f\"3. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"4. Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"5. Batch size: {batch_size}\")\n",
    "print(f\"6. Training samples: {len(train_dataset)}\")\n",
    "print(f\"7. Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print_memory_usage(\"After setup\")\n",
    "print(\"READY FOR TRAINING!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b5de7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 14: TRAINING - SIMPLE VERSION\n",
      "============================================================\n",
      "Starting training...\n",
      "Training samples: 71925\n",
      "Batch size: 8\n",
      "Total batches per epoch: 8991\n",
      "\n",
      "üìÖ Epoch 1/3\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m optimizer.zero_grad()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Calculate loss \u001b[39;00m\n\u001b[32m     43\u001b[39m loss = criterion(outputs, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mVerySimpleProteinCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     89\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[32m     93\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv(x))\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "print_step_header(14, \"TRAINING - SIMPLE VERSION\")\n",
    "\n",
    "# ============================================\n",
    "# PART 1: TRAINING SETUP\n",
    "# ============================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Total batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Create lists to track progress\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# How many times to go through all data\n",
    "num_epochs = 3  \n",
    "best_loss = float('inf')  \n",
    "\n",
    "# ============================================\n",
    "# PART 2: THE TRAINING LOOP\n",
    "# ============================================\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüìÖ Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # ===== TRAINING PHASE =====\n",
    "    model.train() \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Get data\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Clear old gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Calculate loss \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Learn from mistakes\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Show progress every 1000 batches\n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # ===== VALIDATION PHASE =====\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # No learning during validation\n",
    "        for batch in val_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # ===== SHOW RESULTS =====\n",
    "    print(f\"\\nüìä Results for Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # ===== SAVE BEST MODEL =====\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"  ‚úÖ Saved better model! (Loss improved to {avg_val_loss:.4f})\")\n",
    "    else:\n",
    "        print(f\"  Model did not improve\")\n",
    "\n",
    "# ============================================\n",
    "# PART 3: TRAINING COMPLETE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úì Best model saved as: best_model.pth\")\n",
    "print(f\"‚úì Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"‚úì Final validation loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 4: QUICK SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìà Training Summary:\")\n",
    "print(f\"  Epochs completed: {num_epochs}\")\n",
    "print(f\"  Training loss started at: {train_losses[0]:.4f}\")\n",
    "print(f\"  Training loss ended at: {train_losses[-1]:.4f}\")\n",
    "if len(train_losses) > 1:\n",
    "    improvement = train_losses[0] - train_losses[-1]\n",
    "    print(f\"  Improvement: {improvement:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
