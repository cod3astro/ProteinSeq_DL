{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49391d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial - Memory Usage: 0.29 GB\n",
      "\n",
      "STARTING PROTEIN GO TERM PROCESSING\n",
      "Start Time: 22:54:16\n",
      "Initial Memory: 0.29 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEXPLANATION:\\n\\n1. initial_mem = print_memory_usage(\"Initial\")\\n   - Captures memory before processing begins.\\n   - Later we can compare:\\n         current_memory - initial_memory\\n   - Helps detect memory leaks.\\n\\n2. time.strftime(\\'%H:%M:%S\\')\\n   - Formats current time as Hour:Minute:Second.\\n\\n3. Logging statements\\n   - Useful for reproducibility.\\n   - Important when running experiments overnight or on servers.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "1. pandas (pd)\n",
    "   - Used for structured/tabular data manipulation.\n",
    "   - Likely used later for reading protein datasets (e.g., CSV with sequences and GO terms).\n",
    "   - Provides functions like:\n",
    "       - read_csv(), sort_values(), unique(), groupby(), reset_index()\n",
    "\n",
    "2. numpy (np)\n",
    "   - Used for numerical operations.\n",
    "   - Deep learning models require numerical arrays.\n",
    "   - Useful for:\n",
    "       - vector operations, encoding labels, reshaping data\n",
    "\n",
    "3. Dataset (from HuggingFace datasets library)\n",
    "   - Converts data into an optimized format for training.\n",
    "   - Often used with transformers or large NLP models.\n",
    "   - If not using HuggingFace Trainer or tokenizers later, this import may be unnecessary.\n",
    "\n",
    "4. Counter\n",
    "   - Counts frequency of elements in a list.\n",
    "   - Very useful for:\n",
    "       - Counting GO term frequencies, Detecting class imbalance\n",
    "\n",
    "5. gc (Garbage Collector)\n",
    "   - Frees memory manually using gc.collect()\n",
    "   - Important when handling large biological datasets.\n",
    "\n",
    "6. time\n",
    "   - Used to measure execution time.\n",
    "\n",
    "7. psutil\n",
    "   - Tracks memory usage of current process, Useful for monitoring RAM consumption.\n",
    "\n",
    "8. os\n",
    "   - Used for operating system utilities.\n",
    "   - Here it is used to get current process ID.\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "\"\"\"\n",
    "1. time.time()\n",
    "   - Returns current time in seconds since epoch.\n",
    "   - Used to calculate total runtime later: total_time = time.time() - start_time\n",
    "\n",
    "2. os.getpid()\n",
    "   - Gets the process ID of the current Python program.\n",
    "\n",
    "3. psutil.Process(pid)\n",
    "   - Attaches a memory-monitoring object to this process.\n",
    "   - Allows access to:\n",
    "       - memory usage, CPU usage, other process stats\n",
    "\"\"\"\n",
    "\n",
    "def print_memory_usage(step_name):\n",
    "    mem_info = process.memory_info()\n",
    "    mem_gb = mem_info.rss / (1024 ** 3)\n",
    "\n",
    "    print(f\"{step_name} - Memory Usage: {mem_gb:.2f} GB\")\n",
    "    return mem_gb\n",
    "\n",
    "\"\"\"\n",
    "Function Purpose:\n",
    "This function checks how much RAM the program is currently using.\n",
    "\n",
    "1. process.memory_info()\n",
    "   - Retrieves memory statistics for current process.\n",
    "   - Returns an object containing:\n",
    "       - rss (Resident Set Size)\n",
    "       - vms (Virtual memory size)\n",
    "\n",
    "2. mem_info.rss\n",
    "   - RSS = actual physical memory being used.\n",
    "   - This is the most relevant measure for memory usage.\n",
    "\n",
    "3. (1024 ** 3)\n",
    "   - Converts bytes to gigabytes.\n",
    "\n",
    "4. print(f\"{step_name} - Memory Usage: {mem_gb:.2f} GB\")\n",
    "   - Displays step name AND memory.\n",
    "   - This fixes earlier version where step_name was unused.\n",
    "\"\"\"\n",
    "\n",
    "def print_step_header(step_num, step_name):\n",
    "    separator = \"=\" * 60\n",
    "\n",
    "    print(\"\\n\" + separator)\n",
    "    print(f\"STEP {step_num}: {step_name}\")\n",
    "    print(separator)\n",
    "\n",
    "\"\"\"\n",
    "1. \"=\" * 60\n",
    "   - Repeats \"=\" 60 times.\n",
    "   - Creates a clear visual separator.\n",
    "\n",
    "2. print(\"\\n\" + separator)\n",
    "   - Adds spacing before new step.\n",
    "\n",
    "3. f\"STEP {step_num}: {step_name}\"\n",
    "   - Displays structured step numbering.\n",
    "\n",
    "Why this is useful:\n",
    "When processing protein datasets, multiple stages exist:\n",
    "    - Loading data, Cleaning, Encoding, Training, Evaluation\n",
    "\n",
    "This prevents confusion during long runs.\n",
    "\"\"\"\n",
    "\n",
    "initial_mem = print_memory_usage(\"Initial\")\n",
    "\n",
    "print(\"\\nSTARTING PROTEIN GO TERM PROCESSING\")\n",
    "print(f\"Start Time: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Initial Memory: {initial_mem:.2f} GB\")\n",
    "\n",
    "\"\"\"\n",
    "1. initial_mem = print_memory_usage(\"Initial\")\n",
    "   - Captures memory before processing begins.\n",
    "   - Later we can compare:\n",
    "         current_memory - initial_memory\n",
    "   - Helps detect memory leaks.\n",
    "\n",
    "2. time.strftime('%H:%M:%S')\n",
    "   - Formats current time as Hour:Minute:Second.\n",
    "\n",
    "3. Logging statements\n",
    "   - Useful for reproducibility.\n",
    "   - Important when running experiments overnight or on servers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d149e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV entries: 105951\n",
      "FASTA sequences: 105951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nEXPLANATION:\\n\\n1. len(labels_df)\\n   - Returns number of rows in DataFrame.\\n   - Equivalent to number of labeled proteins.\\n\\n2. len(sequence_dict)\\n   - Returns number of protein sequences parsed.\\n\\nWhy this is important:\\nWe must verify that:\\n    number of labels ≈ number of sequences\\n\\nIf they differ significantly:\\n    - Some proteins may lack sequences\\n    - Some sequences may lack labels\\n    - IDs may not match\\n\\nLater, we must ensure proper merging.\\n\\nIMPORTANT NEXT STEP (very important for deep learning):\\n\\nWe must check:\\n    set(labels_df['protein_id_column']) \\n        ∩ \\n    set(sequence_dict.keys())\\n\\nIf intersection is smaller than expected, we may silently lose data.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "\"\"\"\n",
    "Bio.SeqIO is a module from the Biopython library used for reading\n",
    "biological sequence files such as FASTA, GenBank, etc.\n",
    "\n",
    "SeqIO can:\n",
    "- Parse FASTA files automatically\n",
    "- Extract IDs and sequences cleanly\n",
    "- Avoid manual string parsing\n",
    "\"\"\"\n",
    "\n",
    "tsv_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/raw/train/uniprotkb_AND_reviewed_true_AND_protein_2025_12_27.tsv\"\n",
    "labels_df = pd.read_csv(tsv_path, sep='\\t')\n",
    "\n",
    "\"\"\"\n",
    "1. pd.read_csv(tsv_path, sep='\\t')\n",
    "\n",
    "   - read_csv() loads tabular data into a pandas DataFrame.\n",
    "   - sep='\\t' tells pandas that the file is TAB-separated.\n",
    "   - TSV = Tab Separated Values.\n",
    "\n",
    "labels_df is a DataFrame (rows × columns).\n",
    "This DataFrame will later be merged with sequences.\n",
    "\"\"\"\n",
    "\n",
    "fasta_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/raw/train/uniprotkb_AND_reviewed_true_AND_protein_2025_12_27.fasta\"\n",
    "sequence_dict = {}\n",
    "\n",
    "for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "    # UniProt FASTA headers look like:\n",
    "    # sp|P12345|PROTEIN_NAME\n",
    "    # We extract the middle ID (P12345)\n",
    "\n",
    "    header = record.id\n",
    "    if \"|\" in header:\n",
    "        protein_id = header.split(\"|\")[1]\n",
    "    else:\n",
    "        protein_id = header.split()[0]\n",
    "    sequence_dict[protein_id] = str(record.seq)\n",
    "\n",
    "\"\"\"\n",
    "1. SeqIO.parse(fasta_path, \"fasta\")\n",
    "\n",
    "   - Reads FASTA file safely.\n",
    "   - Automatically handles:\n",
    "        - Multi-line sequences\n",
    "        - Header parsing\n",
    "        - End-of-file cases\n",
    "\n",
    "2. record.id\n",
    "   - Returns FASTA header identifier.\n",
    "   - Example:\n",
    "        sp|P12345|PROTEIN_NAME\n",
    "\n",
    "3. header.split(\"|\")[1]\n",
    "   - Splits string at \"|\".\n",
    "   - Index 1 extracts middle ID (P12345).\n",
    "   - This matches the original logic.\n",
    "\n",
    "4. sequence_dict[protein_id] = str(record.seq)\n",
    "   - record.seq is a Seq object.\n",
    "   - Convert to string for easier handling later.\n",
    "\n",
    "Result:\n",
    "sequence_dict:\n",
    "    {\n",
    "        \"P12345\": \"MTEYKLVVVGAGGVGKS...\",\n",
    "        ...\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"TSV entries: {len(labels_df)}\")\n",
    "print(f\"FASTA sequences: {len(sequence_dict)}\")\n",
    "\n",
    "\"\"\"\n",
    "1. len(labels_df)\n",
    "   - Returns number of rows in DataFrame.\n",
    "   - Equivalent to number of labeled proteins.\n",
    "\n",
    "2. len(sequence_dict)\n",
    "   - Returns number of protein sequences parsed.\n",
    "\n",
    "Why this is important:\n",
    "We must verify that:\n",
    "    number of labels ≈ number of sequences\n",
    "\n",
    "If they differ significantly:\n",
    "    - Some proteins may lack sequences\n",
    "    - Some sequences may lack labels\n",
    "    - IDs may not match\n",
    "\n",
    "Later, we must ensure proper merging.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebc940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIMPORTANT:\\n\\nBecause each protein can have MULTIPLE GO terms,\\nthe task is:\\n    MULTI-LABEL CLASSIFICATION\\n\\nThis changes everything downstream:\\n\\n1. We CANNOT use:\\n       CrossEntropyLoss\\n   Because that assumes ONE label per sample.\\n\\n2. We MUST use:\\n       BCEWithLogitsLoss (if PyTorch)\\n   or\\n       BinaryCrossentropy (if Keras)\\n\\n3. The output layer must be:\\n       number_of_unique_GO_terms neurons\\n\\n4. Final activation should be:\\n       Sigmoid (not Softmax)\\n\\nSoftmax = probabilities sum to 1 (single-label)\\nSigmoid = independent probabilities per class (multi-label)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_go_terms(go_string):\n",
    "    \"\"\"\n",
    "    Convert a semicolon-separated GO string into a Python list.\n",
    "\n",
    "    Example:\n",
    "        Input  → \"GO:0008150;GO:0003674\"\n",
    "        Output → [\"GO:0008150\", \"GO:0003674\"]\n",
    "\n",
    "    This is required because:\n",
    "    - The dataset stores multiple GO annotations in one cell.\n",
    "    - Deep learning models cannot work with raw strings.\n",
    "    - We must convert them into structured format (list).\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(go_string) or go_string == \"\":\n",
    "        return []\n",
    "\n",
    "    \"\"\"\n",
    "    1. pd.isna(go_string)\n",
    "       - Checks whether value is NaN (missing).\n",
    "       - NaN can appear if a protein has no GO annotations.\n",
    "\n",
    "    2. go_string == \"\"\n",
    "       - Handles empty strings.\n",
    "       - Sometimes TSV files store missing values as \"\" instead of NaN.\n",
    "\n",
    "    Why return [] instead of None?\n",
    "       - Returning an empty list keeps data type consistent.\n",
    "       - Later, we will expect a LIST of GO terms.\n",
    "       - Returning None would break later loops.\n",
    "\n",
    "    Deep learning perspective:\n",
    "       If we are doing multi-label classification,\n",
    "       returning [] means this protein has no labels.\n",
    "       We must later decide:\n",
    "           - Remove such proteins or Keep them?\n",
    "    \"\"\"\n",
    "\n",
    "    return [term.strip() for term in str(go_string).split(';')]\n",
    "\n",
    "    \"\"\"\n",
    "    1. str(go_string)\n",
    "       - Ensures the value is treated as string.\n",
    "       - Defensive programming in case data type is unexpected.\n",
    "\n",
    "    2. .split(';')\n",
    "       - Splits string at each semicolon.\n",
    "       - Example:\n",
    "           \"GO:0003953; GO:0007165\"\n",
    "       becomes:\n",
    "           [\"GO:0003953\", \" GO:0007165\"]\n",
    "\n",
    "    3. term.strip()\n",
    "       - Removes leading/trailing whitespace.\n",
    "       - Important because some entries have space after semicolon.\n",
    "       - Example: \" GO:0007165\" → \"GO:0007165\"\n",
    "\n",
    "    4. List comprehension\n",
    "       - Efficient way to apply transformation to each term.\n",
    "       - Cleaner than using a for-loop.\n",
    "\n",
    "    FINAL RESULT: A Python list of GO term strings.\n",
    "\n",
    "    IMPORTANT CONCEPTUAL NOTE:\n",
    "        We are converting from:\n",
    "            Multi-label string format\n",
    "        into:\n",
    "            Structured multi-label list format.\n",
    "\n",
    "        This means the problem is NOT single-class classification, It is multi-label classification.\n",
    "    \"\"\"\n",
    "\n",
    "labels_df['go_terms_list'] = labels_df['Gene Ontology IDs'].apply(parse_go_terms)\n",
    "\n",
    "\"\"\"\n",
    "1. labels_df['Gene Ontology IDs']\n",
    "   - Selects the column containing raw GO strings.\n",
    "\n",
    "2. .apply(parse_go_terms)\n",
    "   - Applies the function to EACH row.\n",
    "   - Equivalent to:\n",
    "         for each row:\n",
    "             parse_go_terms(value)\n",
    "   But much cleaner and vectorized.\n",
    "\n",
    "3. labels_df['go_terms_list'] = ...\n",
    "   - Creates a NEW column.\n",
    "   - Does not overwrite original data.\n",
    "   - Good practice for traceability.\n",
    "\n",
    "Now your DataFrame contains:\n",
    "\n",
    "Column: 'Gene Ontology IDs'\n",
    "    → Raw string format\n",
    "\n",
    "Column: 'go_terms_list'\n",
    "    → Python list format\n",
    "\n",
    "Example:\n",
    "\n",
    "Before:\n",
    "    \"GO:0003953; GO:0007165\"\n",
    "After:\n",
    "    [\"GO:0003953\", \"GO:0007165\"]\n",
    "\n",
    "This structured list is REQUIRED for:\n",
    "    - Counting frequencies (Counter), Building label vocabulary, Multi-hot encoding, Training neural networks\n",
    "\n",
    "    IMPORTANT:\n",
    "\n",
    "Because each protein can have MULTIPLE GO terms,\n",
    "the task is: MULTI-LABEL CLASSIFICATION\n",
    "\n",
    "This changes everything downstream:\n",
    "\n",
    "1. We CANNOT use:\n",
    "       CrossEntropyLoss, Because that assumes ONE label per sample.\n",
    "\n",
    "2. We MUST use:\n",
    "       BCEWithLogitsLoss (if PyTorch) or BinaryCrossentropy (if Keras)\n",
    "\n",
    "3. The output layer must be:\n",
    "       number_of_unique_GO_terms neurons\n",
    "\n",
    "4. Final activation should be:\n",
    "       Sigmoid (not Softmax)\n",
    "\n",
    "Softmax = probabilities sum to 1 (single-label)\n",
    "Sigmoid = independent probabilities per class (multi-label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3810254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology IDs</th>\n",
       "      <th>go_terms_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>ABTIR_ACIB9</td>\n",
       "      <td>2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...</td>\n",
       "      <td>Acinetobacter baumannii (strain 1295743)</td>\n",
       "      <td>MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...</td>\n",
       "      <td>GO:0003953; GO:0007165; GO:0019677; GO:0050135...</td>\n",
       "      <td>[GO:0003953, GO:0007165, GO:0019677, GO:005013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>ENG1_RHIMI</td>\n",
       "      <td>Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...</td>\n",
       "      <td>Rhizomucor miehei</td>\n",
       "      <td>MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...</td>\n",
       "      <td>GO:0000272; GO:0005576; GO:0042973; GO:0052861...</td>\n",
       "      <td>[GO:0000272, GO:0005576, GO:0042973, GO:005286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024B7W1</td>\n",
       "      <td>POLG_ZIKVF</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Zika virus (isolate ZIKV/Human/French Polynesi...</td>\n",
       "      <td>MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...</td>\n",
       "      <td>GO:0003724; GO:0003725; GO:0003968; GO:0004252...</td>\n",
       "      <td>[GO:0003724, GO:0003725, GO:0003968, GO:000425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A024RXP8</td>\n",
       "      <td>GUX1_HYPJR</td>\n",
       "      <td>Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "      <td>MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...</td>\n",
       "      <td>GO:0005576; GO:0016162; GO:0030245; GO:0030248</td>\n",
       "      <td>[GO:0005576, GO:0016162, GO:0030245, GO:0030248]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>CUTI1_HYPJR</td>\n",
       "      <td>Cutinase (EC 3.1.1.74)</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "      <td>MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...</td>\n",
       "      <td>GO:0005576; GO:0016052; GO:0050525</td>\n",
       "      <td>[GO:0005576, GO:0016052, GO:0050525]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry   Entry Name                                      Protein names  \\\n",
       "0  A0A009IHW8  ABTIR_ACIB9  2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...   \n",
       "1  A0A023I7E1   ENG1_RHIMI  Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...   \n",
       "2  A0A024B7W1   POLG_ZIKVF  Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "3  A0A024RXP8   GUX1_HYPJR  Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...   \n",
       "4  A0A024SC78  CUTI1_HYPJR                             Cutinase (EC 3.1.1.74)   \n",
       "\n",
       "                                            Organism  \\\n",
       "0           Acinetobacter baumannii (strain 1295743)   \n",
       "1                                  Rhizomucor miehei   \n",
       "2  Zika virus (isolate ZIKV/Human/French Polynesi...   \n",
       "3  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...   \n",
       "4  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...   \n",
       "\n",
       "                                            Sequence  \\\n",
       "0  MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
       "1  MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
       "2  MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...   \n",
       "3  MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...   \n",
       "4  MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...   \n",
       "\n",
       "                                   Gene Ontology IDs  \\\n",
       "0  GO:0003953; GO:0007165; GO:0019677; GO:0050135...   \n",
       "1  GO:0000272; GO:0005576; GO:0042973; GO:0052861...   \n",
       "2  GO:0003724; GO:0003725; GO:0003968; GO:0004252...   \n",
       "3     GO:0005576; GO:0016162; GO:0030245; GO:0030248   \n",
       "4                 GO:0005576; GO:0016052; GO:0050525   \n",
       "\n",
       "                                       go_terms_list  \n",
       "0  [GO:0003953, GO:0007165, GO:0019677, GO:005013...  \n",
       "1  [GO:0000272, GO:0005576, GO:0042973, GO:005286...  \n",
       "2  [GO:0003724, GO:0003725, GO:0003968, GO:000425...  \n",
       "3   [GO:0005576, GO:0016162, GO:0030245, GO:0030248]  \n",
       "4               [GO:0005576, GO:0016052, GO:0050525]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b21ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched proteins: 105951\n",
      "Saved to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/training_data_combined.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEXPLANATION:\\n\\n1. to_csv(output_path, index=False)\\n\\n   - Saves processed dataset to disk.\\n   - index=False prevents saving row indices.\\n   - Ensures clean, reusable CSV file.\\n\\nWhy this step is important:\\n\\n- FASTA parsing is computationally expensive.\\n- Merging datasets repeatedly wastes time.\\n- Saving intermediate dataset improves reproducibility.\\n- This file becomes the starting point for:\\n        tokenization\\n        label encoding\\n        model training\\n\\nThis concludes construction of the unified training dataset.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = labels_df[labels_df['Entry'].isin(sequence_dict.keys())].copy()\n",
    "\n",
    "\"\"\"\n",
    "In this section, we perform a vectorized matching operation between:\n",
    "\n",
    "    - Protein accession IDs from the label DataFrame\n",
    "    - Keys of sequence_dict (which contain available FASTA sequences)\n",
    "\n",
    "Step-by-step explanation:\n",
    "\n",
    "1. labels_df['Entry']\n",
    "   - This selects the column containing UniProt accession IDs.\n",
    "   - Each value represents one protein.\n",
    "\n",
    "2. .isin(sequence_dict.keys())\n",
    "   - The isin() function checks whether each element of the column\n",
    "     exists inside a given collection.\n",
    "   - Here, the collection is sequence_dict.keys(), which contains\n",
    "     all protein IDs parsed from the FASTA file.\n",
    "\n",
    "   Conceptually, this creates a Boolean mask:\n",
    "        True  → accession exists in sequence_dict\n",
    "        False → accession does not exist\n",
    "\n",
    "3. labels_df[ ... ]\n",
    "   - We use this Boolean mask to filter rows.\n",
    "   - This keeps only proteins that have corresponding sequences.\n",
    "\n",
    "4. .copy()\n",
    "   - Creates a deep copy of the filtered data.\n",
    "   - This prevents SettingWithCopyWarning later.\n",
    "   - It ensures modifications do not affect the original DataFrame.\n",
    "\n",
    "Conceptual interpretation:\n",
    "\n",
    "This operation is equivalent to performing an INNER JOIN between:\n",
    "    labels_df (TSV data)\n",
    "and\n",
    "    sequence_dict (FASTA data)\n",
    "\n",
    "However, instead of looping row-by-row, we use pandas' vectorized operations, which are significantly faster and more memory-efficient.\n",
    "\"\"\"\n",
    "\n",
    "filtered_df['sequence'] = filtered_df['Entry'].map(sequence_dict)\n",
    "\n",
    "\"\"\"\n",
    "Now that we have filtered proteins that exist in sequence_dict, we attach their sequences directly into the DataFrame.\n",
    "\n",
    "1. .map(sequence_dict)\n",
    "\n",
    "   - The map() function replaces each value in a Series\n",
    "     using a dictionary lookup.\n",
    "   - For each accession ID in filtered_df['Entry']:\n",
    "         sequence_dict[accession] is retrieved.\n",
    "\n",
    "   Example:\n",
    "        If accession = \"A0A009IHW8\"\n",
    "        Then:\n",
    "            sequence_dict[\"A0A009IHW8\"] → \"MSLEQKKGADIISKIL...\"\n",
    "\n",
    "2. filtered_df['sequence'] = ...\n",
    "   - Creates a new column called 'sequence'\n",
    "   - Each row now contains:\n",
    "         accession\n",
    "         original metadata\n",
    "         parsed GO term list\n",
    "         full amino acid sequence\n",
    "\n",
    "Why this is efficient:\n",
    "\n",
    "- Dictionary lookup is O(1) on average.\n",
    "- No explicit Python loops.\n",
    "- Fully vectorized within pandas.\n",
    "\"\"\"\n",
    "\n",
    "train_df = filtered_df[['Entry', 'sequence', 'go_terms_list', 'Organism']].rename(\n",
    "    columns={\n",
    "        'Entry': 'accession',\n",
    "        'go_terms_list': 'go_terms',\n",
    "        'Organism': 'organism'\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "In this section, we reduce the DataFrame to only essential columns\n",
    "required for model training.\n",
    "\n",
    "1. filtered_df[['Entry', 'sequence', 'go_terms_list', 'Organism']]\n",
    "\n",
    "   - Selects only necessary columns.\n",
    "   - Removes irrelevant metadata (protein names, etc.).\n",
    "   - This reduces memory usage and improves clarity.\n",
    "\n",
    "2. .rename(columns={...})\n",
    "\n",
    "   - Renames columns to cleaner, standardized names.\n",
    "   - 'Entry' → 'accession'\n",
    "   - 'go_terms_list' → 'go_terms'\n",
    "   - 'Organism' → 'organism'\n",
    "\n",
    "Why renaming is important:\n",
    "\n",
    "- Improves readability\n",
    "- Makes downstream code cleaner\n",
    "- Avoids repeatedly referencing long column names\n",
    "\n",
    "After this step, train_df contains:\n",
    "   accession | sequence | go_terms | organism\n",
    "This becomes our core training dataset.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Matched proteins: {len(train_df)}\")\n",
    "\n",
    "\"\"\"\n",
    "1. len(train_df)\n",
    "\n",
    "   - Returns number of successfully matched proteins.\n",
    "   - This equals the number of proteins that have:\n",
    "         ✔ GO annotations\n",
    "         ✔ Available amino acid sequence\n",
    "\n",
    "Sanity check:\n",
    "\n",
    "This number should be close to:\n",
    "        min(len(labels_df), len(sequence_dict))\n",
    "\n",
    "If it is significantly lower,\n",
    "this indicates ID mismatches between TSV and FASTA files.\n",
    "\n",
    "This step confirms integrity of data merging.\n",
    "\"\"\"\n",
    "\n",
    "output_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/training_data_combined.csv\"\n",
    "train_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "\"\"\"\n",
    "1. to_csv(output_path, index=False)\n",
    "\n",
    "   - Saves processed dataset to disk.\n",
    "   - index=False prevents saving row indices.\n",
    "   - Ensures clean, reusable CSV file.\n",
    "\n",
    "Why this step is important:\n",
    "\n",
    "- FASTA parsing is computationally expensive.\n",
    "- Merging datasets repeatedly wastes time.\n",
    "- Saving intermediate dataset improves reproducibility.\n",
    "- This file becomes the starting point for:\n",
    "        tokenization\n",
    "        label encoding\n",
    "        model training\n",
    "\n",
    "This concludes construction of the unified training dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f281b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>sequence</th>\n",
       "      <th>go_terms</th>\n",
       "      <th>organism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...</td>\n",
       "      <td>[GO:0003953, GO:0007165, GO:0019677, GO:005013...</td>\n",
       "      <td>Acinetobacter baumannii (strain 1295743)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...</td>\n",
       "      <td>[GO:0000272, GO:0005576, GO:0042973, GO:005286...</td>\n",
       "      <td>Rhizomucor miehei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024B7W1</td>\n",
       "      <td>MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...</td>\n",
       "      <td>[GO:0003724, GO:0003725, GO:0003968, GO:000425...</td>\n",
       "      <td>Zika virus (isolate ZIKV/Human/French Polynesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A024RXP8</td>\n",
       "      <td>MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...</td>\n",
       "      <td>[GO:0005576, GO:0016162, GO:0030245, GO:0030248]</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...</td>\n",
       "      <td>[GO:0005576, GO:0016052, GO:0050525]</td>\n",
       "      <td>Hypocrea jecorina (strain ATCC 56765 / BCRC 32...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accession                                           sequence  \\\n",
       "0  A0A009IHW8  MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
       "1  A0A023I7E1  MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
       "2  A0A024B7W1  MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...   \n",
       "3  A0A024RXP8  MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...   \n",
       "4  A0A024SC78  MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...   \n",
       "\n",
       "                                            go_terms  \\\n",
       "0  [GO:0003953, GO:0007165, GO:0019677, GO:005013...   \n",
       "1  [GO:0000272, GO:0005576, GO:0042973, GO:005286...   \n",
       "2  [GO:0003724, GO:0003725, GO:0003968, GO:000425...   \n",
       "3   [GO:0005576, GO:0016162, GO:0030245, GO:0030248]   \n",
       "4               [GO:0005576, GO:0016052, GO:0050525]   \n",
       "\n",
       "                                            organism  \n",
       "0           Acinetobacter baumannii (strain 1295743)  \n",
       "1                                  Rhizomucor miehei  \n",
       "2  Zika virus (isolate ZIKV/Human/French Polynesi...  \n",
       "3  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...  \n",
       "4  Hypocrea jecorina (strain ATCC 56765 / BCRC 32...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins: 105951\n",
      "\n",
      "Collecting unique GO terms and computing frequencies...\n",
      "Found 27615 unique GO terms total\n",
      "After collecting all terms - Memory Usage: 0.55 GB\n",
      "\n",
      "Selected 2000 frequent + 761 random = 2761 total GO terms\n",
      "Most common term: GO:0005737 (appears 22662 times)\n",
      "Sample random term: GO:0016311 (appears 62 times)\n",
      "After term selection - Memory Usage: 0.55 GB\n",
      "Time elapsed: 10.3 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEXPLANATION:\\n\\nWe combine:\\n    2000 frequent terms\\n+    761 random learnable terms\\n--------------------------------\\n=   2761 total GO terms\\n\\nThis defines the final output dimension of the model.\\n\\nIf we later perform multi-hot encoding,\\nthe output layer size will be:\\n    len(top_go_terms)\\n\\nThis block also prints:\\n    - Frequency sanity check\\n    - Memory usage\\n    - Runtime tracking\\n\\nThis completes label vocabulary construction.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total proteins: {len(train_df)}\")\n",
    "\n",
    "\"\"\"\n",
    "We begin by reporting the number of proteins available after merging sequences with GO annotations.\n",
    "This number represents the total samples that can potentially be used for training.\n",
    "This establishes baseline dataset size before label filtering.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nCollecting unique GO terms and computing frequencies...\")\n",
    "go_counts = Counter(term for go_list in train_df['go_terms'] for term in go_list)\n",
    "all_go_terms = set(go_counts.keys())\n",
    "\n",
    "print(f\"Found {len(all_go_terms)} unique GO terms total\")\n",
    "mem1 = print_memory_usage(\"After collecting all terms\")\n",
    "\n",
    "\"\"\"\n",
    "In this section, we compute BOTH:\n",
    "    - The set of unique GO terms\n",
    "    - The frequency of each GO term\n",
    "\n",
    "Counter(term for go_list in train_df['go_terms'] for term in go_list)\n",
    "Explanation:\n",
    "\n",
    "- Outer loop iterates through each protein’s GO list.\n",
    "- Inner loop iterates through each GO term in that list.\n",
    "- Each term is fed into Counter.\n",
    "- Counter automatically counts frequency.\n",
    "\n",
    "This accomplishes:\n",
    "    ✔ Unique term extraction, Frequency counting, Single pass over data\n",
    "\n",
    "Computationally cleaner and more efficient.\n",
    "\n",
    "all_go_terms = set(go_counts.keys())\n",
    "- Extracts unique GO terms.\n",
    "- Equivalent to previous manual set update.\n",
    "This removes repetition while keeping identical results.\n",
    "\"\"\"\n",
    "\n",
    "ultra_general = {'GO:0008150', 'GO:0005575', 'GO:0003674'}\n",
    "\n",
    "filtered_terms = [\n",
    "    term for term, count in go_counts.most_common()\n",
    "    if term not in ultra_general\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Gene Ontology has three root terms:\n",
    "\n",
    "    GO:0008150 → Biological Process\n",
    "    GO:0005575 → Cellular Component\n",
    "    GO:0003674 → Molecular Function\n",
    "\n",
    "These terms are extremely general, Nearly every protein inherits from them.\n",
    "\n",
    "Including them would:\n",
    "    - Add no learning value\n",
    "    - Inflate class counts\n",
    "    - Bias predictions\n",
    "We remove them before selection.\n",
    "\n",
    "go_counts.most_common()\n",
    "- Returns list of (term, count) pairs sorted by descending frequency.\n",
    "- This is important because:\n",
    "    We want the most frequent terms first.\n",
    "\n",
    "We extract only the term from each (term, count) tuple.\n",
    "This preserves frequency order.\n",
    "\n",
    "At this stage:\n",
    "filtered_terms is ordered from most frequent → least frequent,\n",
    "excluding ultra-general terms.\n",
    "\"\"\"\n",
    "\n",
    "top_n_frequent = 2000\n",
    "top_go_frequent = filtered_terms[:top_n_frequent]\n",
    "\n",
    "\"\"\"\n",
    "We select the 700 most frequent GO terms.\n",
    "\n",
    "Why frequent terms?\n",
    "\n",
    "- Frequent classes have enough examples to train reliably.\n",
    "- Rare classes can cause unstable gradients.\n",
    "- Deep learning performs poorly on extremely sparse classes.\n",
    "\n",
    "This ensures strong core signal in training.\n",
    "\n",
    "This slice operation preserves order because:\n",
    "most_common() already sorted by frequency.\n",
    "\"\"\"\n",
    "\n",
    "remaining_terms = filtered_terms[top_n_frequent:]\n",
    "\n",
    "remaining_filtered = [\n",
    "    term for term in remaining_terms\n",
    "    if go_counts[term] >= 5\n",
    "]\n",
    "np.random.seed(42)\n",
    "\n",
    "top_go_random = (\n",
    "    np.random.choice(remaining_filtered, 761, replace=False).tolist()\n",
    "    if len(remaining_filtered) >= 761\n",
    "    else remaining_filtered\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "We now add diversity by selecting additional terms.\n",
    "\n",
    "Why random terms?\n",
    "\n",
    "If we only select frequent terms:\n",
    "    - Model becomes biased toward common functions.\n",
    "    - Rare but meaningful functions are ignored.\n",
    "\n",
    "We apply a learnability threshold:\n",
    "    go_counts[term] >= 5\n",
    "\n",
    "This ensures:\n",
    "    - Each selected term appears at least 3 times.\n",
    "    - Avoids selecting extremely rare terms (1–2 samples).\n",
    "\n",
    "Random selection:\n",
    "    np.random.choice(..., replace=False)\n",
    "\n",
    "- replace=False ensures no duplicates.\n",
    "- seed=42 ensures reproducibility.\n",
    "- Every run produces identical selection.\n",
    "\n",
    "If fewer than 761 valid terms exist,\n",
    "we simply take all available ones.\n",
    "\n",
    "This logic is identical to original code,\n",
    "but slightly cleaner.\n",
    "\"\"\"\n",
    "\n",
    "top_go_terms = top_go_frequent + top_go_random\n",
    "\n",
    "print(f\"\\nSelected {len(top_go_frequent)} frequent + \"\n",
    "      f\"{len(top_go_random)} random = {len(top_go_terms)} total GO terms\")\n",
    "\n",
    "print(f\"Most common term: {top_go_frequent[0]} \"\n",
    "      f\"(appears {go_counts[top_go_frequent[0]]} times)\")\n",
    "\n",
    "if top_go_random:\n",
    "    print(f\"Sample random term: {top_go_random[0]} \"\n",
    "          f\"(appears {go_counts[top_go_random[0]]} times)\")\n",
    "\n",
    "mem2 = print_memory_usage(\"After term selection\")\n",
    "\n",
    "print(f\"Time elapsed: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "\"\"\"\n",
    "We combine:\n",
    "    2000 frequent terms\n",
    "+    761 random learnable terms\n",
    "--------------------------------\n",
    "=   2761 total GO terms\n",
    "\n",
    "This defines the final output dimension of the model.\n",
    "\n",
    "If we later perform multi-hot encoding,\n",
    "the output layer size will be:\n",
    "    len(top_go_terms)\n",
    "\n",
    "This block also prints:\n",
    "    - Frequency sanity check\n",
    "    - Memory usage\n",
    "    - Runtime tracking\n",
    "\n",
    "This completes label vocabulary construction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f4ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins: 105,951\n",
      "Total selected GO terms: 2,761\n",
      "Created binary DataFrame with 2,761 label columns\n",
      "After binary matrix creation - Memory Usage: 0.69 GB\n",
      "Matrix filling time: 1.7 seconds\n",
      "Total time elapsed: 12.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# We create a mapping from GO term → column index, This allows fast assignment without looping over all labels repeatedly.\n",
    "go_to_index = {go: idx for idx, go in enumerate(top_go_terms)}\n",
    "\n",
    "num_proteins = len(train_df)\n",
    "num_labels = len(top_go_terms)\n",
    "\n",
    "print(f\"Total proteins: {num_proteins:,}\")\n",
    "print(f\"Total selected GO terms: {num_labels:,}\")\n",
    "\n",
    "# we create one full matrix at once using NumPy, Using int8 keeps memory small (1 byte per entry).\n",
    "\n",
    "binary_matrix = np.zeros((num_proteins, num_labels), dtype=np.int8)\n",
    "\n",
    "# At this stage, we already created:\n",
    "# 1. go_to_index → mapping of GO term → column position\n",
    "# 2. binary_matrix → a 2D NumPy array initialized with zeros\n",
    "#\n",
    "# Shape of binary_matrix:\n",
    "# (number_of_proteins, number_of_selected_GO_terms)\n",
    "#\n",
    "# Each row represents ONE protein, Each column represents ONE GO term label.\n",
    "# A value of 1 means the protein has that GO term, A value of 0 means it does not.\n",
    "#\n",
    "# We now populate this matrix.\n",
    "fill_start_time = time.time()\n",
    "\n",
    "# enumerate() gives us:\n",
    "# protein_idx → integer row index (0, 1, 2, ...)\n",
    "# go_list     → list of GO terms for that protein\n",
    "#\n",
    "# We use enumerate because we must know exactly, which row in the matrix corresponds to each protein.\n",
    "\n",
    "for protein_idx, go_list in enumerate(train_df['go_terms']):\n",
    "\n",
    "    # go_list is something like: ['GO:0003953', 'GO:0007165', 'GO:0019677', ...]\n",
    "    #\n",
    "    # Instead of checking every possible label (which would be slow),\n",
    "    # we only iterate through the GO terms that actually exist, for this protein.\n",
    "\n",
    "    for go in go_list:\n",
    "\n",
    "        # We only care about GO terms that are inside our selected label set (top_go_terms).\n",
    "        #\n",
    "        # go_to_index is a dictionary:\n",
    "        #   key   → GO term string, value → integer column index\n",
    "        #\n",
    "        # Dictionary lookup is O(1) (constant time),\n",
    "        # which makes this approach very efficient.\n",
    "\n",
    "        if go in go_to_index:\n",
    "\n",
    "            # Retrieve the column index corresponding to this GO term.\n",
    "            label_idx = go_to_index[go]\n",
    "\n",
    "            # We now update the binary matrix.\n",
    "            #\n",
    "            # protein_idx → row position (which protein)\n",
    "            # label_idx   → column position (which GO term)\n",
    "            #\n",
    "            # Setting value to 1 indicates that this protein is annotated with this GO term.\n",
    "\n",
    "            binary_matrix[protein_idx, label_idx] = 1\n",
    "\n",
    "# At this point, binary_matrix is a NumPy array.\n",
    "# While NumPy arrays are memory efficient, they do not carry column names.\n",
    "# For interpretability and easier downstream processing (saving to CSV, debugging, inspecting specific labels), we convert it into a Pandas DataFrame.\n",
    "\n",
    "# We create column names using the original GO term list.\n",
    "# Each column will be named:  label_GO:000xxxx\n",
    "#\n",
    "# This makes it clear that these columns represent binary label indicators.\n",
    "\n",
    "column_names = [f\"label_{go}\" for go in top_go_terms]\n",
    "\n",
    "# Now we create the DataFrame.\n",
    "# Each column corresponds exactly to the index position defined earlier in go_to_index.\n",
    "\n",
    "binary_df = pd.DataFrame(binary_matrix, columns=column_names)\n",
    "\n",
    "# The shape of this DataFrame is: (number_of_proteins, number_of_selected_GO_terms)\n",
    "# Example:\n",
    "# Protein 1 → [0, 1, 0, 0, 1, ...]\n",
    "# Protein 2 → [1, 0, 0, 1, 0, ...]\n",
    "#\n",
    "# This structure is known as a MULTI-LABEL BINARY MATRIX.\n",
    "# It is the standard format required for multi-label classification.\n",
    "\n",
    "print(f\"Created binary DataFrame with {binary_df.shape[1]:,} label columns\")\n",
    "\n",
    "del binary_matrix\n",
    "gc.collect()\n",
    "mem3 = print_memory_usage(\"After binary matrix creation\")\n",
    "\n",
    "print(f\"Matrix filling time: {time.time() - fill_start_time:.1f} seconds\")\n",
    "print(f\"Total time elapsed: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d979e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: 105,951 proteins × 2,763 columns\n",
      "\n",
      "VERIFICATION:\n",
      "Example protein: A0A009IHW8\n",
      "Sequence length: 269\n",
      "Number of GO term labels: 2,761\n",
      "Positive labels for first protein: 2 out of 2761\n",
      "All labels have at least one positive example\n",
      "After final dataframe - Memory Usage: 0.64 GB\n",
      "Total time elapsed: 17.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# At this stage:\n",
    "# - train_df contains original protein metadata (sequence, accession, etc.)\n",
    "# - binary_df contains the multi-label binary matrix\n",
    "#\n",
    "# Both DataFrames have the same number of rows and are aligned in the same order.\n",
    "# Then we concatenate them horizontally, axis=1 means column-wise concatenation.\n",
    "\n",
    "final_df = pd.concat(\n",
    "    [\n",
    "        train_df[['sequence', 'accession']],  # original features\n",
    "        binary_df                              # binary label matrix\n",
    "    ], axis=1\n",
    ")\n",
    "print(f\"Final DataFrame shape: {final_df.shape[0]:,} proteins × {final_df.shape[1]:,} columns\")\n",
    "\n",
    "# After concatenation, binary_df is no longer needed, Deleting it frees memory immediately.\n",
    "del binary_df\n",
    "gc.collect()\n",
    "\n",
    "# We verify that:\n",
    "# 1. Accession ID is correct\n",
    "# 2. Sequence exists and has realistic length\n",
    "# 3. Label columns exist and are correctly counted\n",
    "\n",
    "print(\"\\nVERIFICATION:\")\n",
    "\n",
    "example_accession = final_df.iloc[0]['accession']\n",
    "example_sequence = final_df.iloc[0]['sequence']\n",
    "\n",
    "print(f\"Example protein: {example_accession}\")\n",
    "print(f\"Sequence length: {len(example_sequence):,}\")\n",
    "\n",
    "# Count label columns\n",
    "label_columns = [col for col in final_df.columns if col.startswith('label_')]\n",
    "print(f\"Number of GO term labels: {len(label_columns):,}\")\n",
    "\n",
    "# For multi-label classification, each protein can have multiple active labels.\n",
    "# We sum across label columns for the first protein.\n",
    "# Because labels are 0/1, sum gives the count of positive labels.\n",
    "\n",
    "positive_counts = final_df.loc[0, label_columns].sum()\n",
    "print(f\"Positive labels for first protein: {positive_counts} out of {len(label_columns)}\")\n",
    "\n",
    "# A label column should not be entirely zero.\n",
    "# That would mean no protein contains that GO term, which would make the label useless for training.\n",
    "\n",
    "empty_labels = [col for col in label_columns if final_df[col].sum() == 0]\n",
    "\n",
    "if empty_labels:\n",
    "    print(f\"Warning: {len(empty_labels)} labels have no positive examples!\")\n",
    "else:\n",
    "    print(\"All labels have at least one positive example\")\n",
    "\n",
    "mem4 = print_memory_usage(\"After final dataframe\")\n",
    "print(f\"Total time elapsed: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f146faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 105,951 proteins\n",
      "Number of label columns: 2,761\n",
      "Conversion time: 28.5 seconds\n",
      "Saving HuggingFace Dataset to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 105951/105951 [31:55<00:00, 55.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n",
      "Saved metadata to: C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/go_terms_metadata.csv\n",
      "Final - Memory Usage: 0.41 GB\n",
      "\n",
      "PERFORMANCE SUMMARY:\n",
      "Total processing time: 1962.2 seconds (32.7 minutes)\n",
      "Memory increase: 0.12 GB\n",
      "Time: 23:26:58\n",
      "\n",
      "DATA SUMMARY:\n",
      "  • Input proteins: 105,951\n",
      "  • Unique GO terms (original): 27,615\n",
      "  • Selected GO terms: 2,761\n",
      "  • Final dataset size: 105,951 × 2,763\n",
      "  • Label representation: multi-hot binary (int8)\n",
      "\n",
      "All steps completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Datasets is built on Apache Arrow, which is memory-efficient and columnar.\n",
    "# Instead of manually chunking and building dictionaries, we directly convert the full DataFrame.\n",
    "#\n",
    "# preserve_index=False prevents the pandas index from becoming an additional column in the dataset.\n",
    "\n",
    "hf_start_time = time.time()\n",
    "\n",
    "dataset = Dataset.from_pandas(final_df, preserve_index=False)\n",
    "print(f\"Dataset created with {len(dataset):,} proteins\")\n",
    "\n",
    "# Count label columns\n",
    "label_columns = [col for col in dataset.column_names if col.startswith('label_')]\n",
    "print(f\"Number of label columns: {len(label_columns):,}\")\n",
    "\n",
    "print(f\"Conversion time: {time.time() - hf_start_time:.1f} seconds\")\n",
    "\n",
    "save_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "print(f\"Saving HuggingFace Dataset to: {save_path}\")\n",
    "\n",
    "# save_to_disk stores dataset in Arrow format\n",
    "# This is much faster to reload later compared to CSV.\n",
    "\n",
    "dataset.save_to_disk(save_path)\n",
    "print(\"Dataset saved successfully.\")\n",
    "\n",
    "# Metadata helps us:\n",
    "# - Track label frequency, Know which labels were frequent vs random, Compute imbalance statistics later\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    'go_term': top_go_terms,\n",
    "    'count': [go_counts[term] for term in top_go_terms],\n",
    "    'percentage': [(go_counts[term] / len(final_df)) * 100 for term in top_go_terms],\n",
    "    'is_frequent': [True] * len(top_go_frequent) + [False] * len(top_go_random)\n",
    "})\n",
    "\n",
    "metadata_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/go_terms_metadata.csv\"\n",
    "metadata.to_csv(metadata_path, index=False)\n",
    "print(f\"Saved metadata to: {metadata_path}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "final_mem = print_memory_usage(\"Final\")\n",
    "\n",
    "print(\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(f\"Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Memory increase: {final_mem - initial_mem:.2f} GB\")\n",
    "print(f\"Time: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nDATA SUMMARY:\")\n",
    "print(f\"  • Input proteins: {len(train_df):,}\")\n",
    "print(f\"  • Unique GO terms (original): {len(all_go_terms):,}\")\n",
    "print(f\"  • Selected GO terms: {len(top_go_terms):,}\")\n",
    "print(f\"  • Final dataset size: {final_df.shape[0]:,} × {final_df.shape[1]:,}\")\n",
    "print(f\"  • Label representation: multi-hot binary (int8)\")\n",
    "\n",
    "print(\"\\nAll steps completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a26e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "After imports - Memory Usage: 0.47 GB\n",
      "✓ Random seed set to: 42\n",
      "After setting seeds - Memory Usage: 0.47 GB\n",
      "Loading dataset...\n",
      "Before loading - Memory Usage: 0.47 GB\n",
      "✓ Loaded: 105,951 proteins\n",
      "After loading - Memory Usage: 0.61 GB\n",
      "✓ Memory used: 0.138 GB\n",
      "✓ Number of GO term labels: 2,761\n",
      "✓ Dataset formatted for PyTorch (labels as tensors)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print_memory_usage(\"After imports\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    We fix all major random generators to ensure reproducibility.\n",
    "    This ensures:\n",
    "    - Same weight initialization\n",
    "    - Same dataset shuffling\n",
    "    - Same training behavior across runs\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # If later using GPU: torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"✓ Random seed set to: {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "print_memory_usage(\"After setting seeds\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "mem_before = print_memory_usage(\"Before loading\")\n",
    "dataset_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "\n",
    "# load_from_disk loads Arrow-formatted dataset efficiently.\n",
    "# This does NOT load everything as Python objects.\n",
    "# It keeps the memory-efficient Arrow backend.\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(f\"✓ Loaded: {len(dataset):,} proteins\")\n",
    "\n",
    "mem_after = print_memory_usage(\"After loading\")\n",
    "print(f\"✓ Memory used: {mem_after - mem_before:.3f} GB\")\n",
    "\n",
    "# Our dataset contains:\n",
    "# - sequence\n",
    "# - accession\n",
    "# - 2700+ label columns (label_GO:XXXXXXX)\n",
    "#\n",
    "# We programmatically detect label columns instead of hardcoding them.\n",
    "\n",
    "label_columns = [col for col in dataset.column_names if col.startswith(\"label_\")]\n",
    "print(f\"✓ Number of GO term labels: {len(label_columns):,}\")\n",
    "\n",
    "# HuggingFace allows us to return tensors directly.\n",
    "#\n",
    "# type=\"torch\" ensures:\n",
    "# - label columns are returned as torch.Tensor\n",
    "# - no pandas conversion\n",
    "# - no duplication in RAM\n",
    "#\n",
    "# We include:\n",
    "# - sequence (still string for now)\n",
    "# - all label columns (as tensors)\n",
    "\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=label_columns  # labels as tensors\n",
    ")\n",
    "print(\"✓ Dataset formatted for PyTorch (labels as tensors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b84a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded\n",
      "Total proteins: 105951\n",
      "\n",
      "📊 Sequence Length Statistics\n",
      "Min length: 2\n",
      "Max length: 35213\n",
      "Mean length: 472.26\n",
      "Median length: 358.0\n",
      "Std deviation: 535.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = \"C:/Users/USER/Documents/cod3astro/ML_AI/ProteinSeq_DL/data/processed/protein_go_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(\"✓ Dataset loaded\")\n",
    "print(f\"Total proteins: {len(dataset)}\")\n",
    "\n",
    "# Compute sequence lengths\n",
    "seq_lengths = [len(seq) for seq in dataset[\"sequence\"]]\n",
    "\n",
    "# Convert to numpy for stats\n",
    "seq_lengths = np.array(seq_lengths)\n",
    "\n",
    "print(\"\\n📊 Sequence Length Statistics\")\n",
    "print(f\"Min length: {seq_lengths.min()}\")\n",
    "print(f\"Max length: {seq_lengths.max()}\")\n",
    "print(f\"Mean length: {seq_lengths.mean():.2f}\")\n",
    "print(f\"Median length: {np.median(seq_lengths)}\")\n",
    "print(f\"Std deviation: {seq_lengths.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Sequence Length Distribution\n",
      "   0 - 250  : 35261 proteins\n",
      " 250 - 500  : 37285 proteins\n",
      " 500 - 750  : 17379 proteins\n",
      " 750 - 1000 : 7145 proteins\n",
      "1000 - 1250 : 3509 proteins\n",
      "1250 - 1500 : 1878 proteins\n",
      "1500 - 1750 : 956 proteins\n",
      "1750 - ∞    : 2538 proteins\n"
     ]
    }
   ],
   "source": [
    "# Create bins every 100 residues up to 2000\n",
    "bins = list(range(0, 2000, 250))  # 0,250,500,...,2000\n",
    "bins.append(float(\"inf\"))         # >2000 bin\n",
    "\n",
    "hist, bin_edges = np.histogram(seq_lengths, bins=bins)\n",
    "\n",
    "print(\"\\n📦 Sequence Length Distribution\")\n",
    "for i in range(len(hist)):\n",
    "    start = int(bin_edges[i])\n",
    "    end = int(bin_edges[i+1]) if bin_edges[i+1] != float(\"inf\") else \"∞\"\n",
    "    print(f\"{start:>4} - {end:<4} : {hist[i]} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Checking for duplicate sequences...\n",
      "Total sequences: 105951\n",
      "Unique sequences: 102750\n",
      "Duplicate sequences: 3201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 Checking for duplicate sequences...\")\n",
    "\n",
    "unique_sequences = set(dataset[\"sequence\"]) # Sequences that occurs once\n",
    "total_sequences = len(dataset) # Total number of all sequences\n",
    "unique_count = len(unique_sequences) # Total number of unoque sequence\n",
    "\n",
    "duplicates = total_sequences - unique_count\n",
    "\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "print(f\"Unique sequences: {unique_count}\")\n",
    "print(f\"Duplicate sequences: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fe6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Removing duplicate sequences (Arrow-safe method)...\n",
      "Original dataset size: 105951\n",
      "Keeping 102750 unique sequences\n",
      "New dataset size: 102750\n",
      "Removed duplicates: 3201\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Removing duplicate sequences (Arrow-safe method)...\")\n",
    "\n",
    "original_size = len(dataset)\n",
    "print(f\"Original dataset size: {original_size}\")\n",
    "\n",
    "# Get sequence column\n",
    "sequences = dataset[\"sequence\"]\n",
    "\n",
    "# Find first occurrence index of each sequence\n",
    "seen = {}\n",
    "keep_indices = []\n",
    "\n",
    "for idx, seq in enumerate(sequences):\n",
    "    if seq not in seen:\n",
    "        seen[seq] = True\n",
    "        keep_indices.append(idx)\n",
    "\n",
    "print(f\"Keeping {len(keep_indices)} unique sequences\")\n",
    "\n",
    "# Select only first occurrences\n",
    "dataset = dataset.select(keep_indices)\n",
    "\n",
    "new_size = len(dataset)\n",
    "print(f\"New dataset size: {new_size}\")\n",
    "print(f\"Removed duplicates: {original_size - new_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2aa98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ProteinDataset class created successfully\n",
      "After creating dataset class - Memory Usage: 0.27 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27440643310546875"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class prepares protein sequences and GO term labels for training in PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, max_seq_length=1024):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace Arrow dataset\n",
    "            max_seq_length: maximum allowed sequence length\n",
    "        \"\"\"\n",
    "\n",
    "        # Store dataset reference\n",
    "        self.dataset = hf_dataset\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Identify GO label columns automatically\n",
    "        self.label_cols = [\n",
    "            col for col in hf_dataset.column_names\n",
    "            if col.startswith(\"label_\")\n",
    "        ]\n",
    "\n",
    "        # Define amino acid vocabulary (20 standard amino acids)\n",
    "        self.amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "        # Create mapping: amino acid → integer index\n",
    "        # 0 reserved for padding / unknown\n",
    "        self.aa_to_idx = {\n",
    "            aa: i + 1 for i, aa in enumerate(self.amino_acids)\n",
    "        }\n",
    "        print(f\"  Found {len(self.label_cols)} GO term labels\")\n",
    "\n",
    "    # Return total number of proteins\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # Get one protein sample\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get raw sequence\n",
    "        sequence = self.dataset[idx][\"sequence\"]\n",
    "\n",
    "        # Encode sequence to numeric form\n",
    "        encoded_seq = self.encode_sequence(sequence)\n",
    "\n",
    "        # Extract GO term labels\n",
    "        labels = [\n",
    "            self.dataset[idx][col]\n",
    "            for col in self.label_cols\n",
    "        ]\n",
    "        # Convert to tensors\n",
    "        return {\n",
    "            \"sequence\": torch.tensor(encoded_seq, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    # Convert amino acid letters → numeric indices\n",
    "    def encode_sequence(self, sequence):\n",
    "\n",
    "        # Truncate if sequence is longer than max length\n",
    "        sequence = sequence[:self.max_seq_length]\n",
    "\n",
    "        # Convert each amino acid to number\n",
    "        encoded = [\n",
    "            self.aa_to_idx.get(aa, 0)  # 0 if unknown\n",
    "            for aa in sequence\n",
    "        ]\n",
    "\n",
    "        # Pad with zeros if sequence is shorter\n",
    "        padding_length = self.max_seq_length - len(encoded)\n",
    "        if padding_length > 0:\n",
    "            encoded += [0] * padding_length\n",
    "\n",
    "        return encoded\n",
    "\n",
    "print(\"✓ ProteinDataset class created successfully\")\n",
    "print_memory_usage(\"After creating dataset class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8519b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before splitting data - Memory Usage: 0.05 GB\n",
      "  Found 2761 GO term labels\n",
      "  Found 2761 GO term labels\n",
      "  Found 2761 GO term labels\n",
      "✓ Data split completed:\n",
      "  Training samples: 71,925\n",
      "  Validation samples: 15,412\n",
      "  Test samples: 15,413\n",
      "After splitting data - Memory Usage: 0.46 GB\n",
      "  Memory used by splits: 0.42 GB\n",
      "  Garbage collection performed\n",
      "After garbage collection - Memory Usage: 0.46 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4638175964355469"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_before = print_memory_usage(\"Before splitting data\")\n",
    "\n",
    "# STEP 1: Split into 70% train and 30% temporary\n",
    "split_1 = dataset.train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset_arrow = split_1[\"train\"]\n",
    "temp_dataset_arrow = split_1[\"test\"]\n",
    "\n",
    "# STEP 2: Split temporary 30% into 15% val + 15% test\n",
    "split_2 = temp_dataset_arrow.train_test_split(\n",
    "    test_size=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_dataset_arrow = split_2[\"train\"]\n",
    "test_dataset_arrow = split_2[\"test\"]\n",
    "\n",
    "# Wrap each split with our PyTorch Dataset class\n",
    "train_dataset = ProteinDataset(train_dataset_arrow, max_seq_length=256)\n",
    "val_dataset = ProteinDataset(val_dataset_arrow, max_seq_length=256)\n",
    "test_dataset = ProteinDataset(test_dataset_arrow, max_seq_length=256)\n",
    "\n",
    "print(\"✓ Data split completed:\")\n",
    "print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "print(f\"  Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"  Test samples: {len(test_dataset):,}\")\n",
    "\n",
    "# Memory tracking\n",
    "mem_after = print_memory_usage(\"After splitting data\")\n",
    "print(f\"  Memory used by splits: {mem_after - mem_before:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"  Garbage collection performed\")\n",
    "print_memory_usage(\"After garbage collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eea5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SimpleProteinCNN class created\n",
      "  Model parameters: 273,385\n",
      "After creating model - Memory Usage: 0.02 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0248870849609375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL DEFINITION\n",
    "\n",
    "# In this section, we define a Convolutional Neural Network (CNN) for multi-label protein function prediction.\n",
    "# The goal is to take an amino acid sequence and predict multiple GO term labels simultaneously.\n",
    "\n",
    "class SimpleProteinCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for multi-label GO term prediction.\n",
    "\n",
    "    Architecture Overview:\n",
    "    1. Embedding layer (converts amino acids to dense vectors)\n",
    "    2. 3 Convolutional layers (extract sequence patterns)\n",
    "    3. Global Max Pooling (captures strongest motif signals)\n",
    "    4. Fully connected layers (combine extracted features)\n",
    "    5. Output layer (one neuron per GO term)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # EMBEDDING LAYER\n",
    "\n",
    "        # We convert amino acid indices into dense vectors.\n",
    "        #\n",
    "        # Neural networks do not work well with raw integers.\n",
    "        # Embeddings allow the model to learn biochemical similarity between amino acids (e.g., hydrophobic residues may learn similar vector representations).\n",
    "        #\n",
    "        # 21 input tokens: 20 standard amino acids, 1 padding/unknown token (index 0)\n",
    "        # 64 = embedding dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=21,\n",
    "            embedding_dim=64,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # CONVOLUTIONAL FEATURE EXTRACTORS\n",
    "\n",
    "        # We use 3 Conv1D layers with increasing receptive fields.\n",
    "        #\n",
    "        # Conv1: kernel_size=3 → captures short motifs (tri-peptides)\n",
    "        # Conv2: kernel_size=5 → medium motifs\n",
    "        # Conv3: kernel_size=7 → longer motifs\n",
    "        #\n",
    "        # Padding ensures output length stays same as input length.\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(128, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # GLOBAL MAX POOLING\n",
    "\n",
    "        # AdaptiveMaxPool1d(1) reduces sequence dimension to 1.\n",
    "        #\n",
    "        # This means:\n",
    "        # From (batch, channels, seq_len)    To   (batch, channels, 1)\n",
    "        #\n",
    "        # It selects the strongest activation across the sequence,\n",
    "        # meaning it keeps the most important detected motif.\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # FULLY CONNECTED CLASSIFIER\n",
    "\n",
    "        # After pooling, we reduce feature space and learn nonlinear label interactions.\n",
    "        #\n",
    "        # Dropout prevents overfitting by randomly disabling neurons during training.\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # This defines how input flows through the network.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x → shape (batch_size, sequence_length)\n",
    "        Output:\n",
    "            logits → shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # STEP 1: Embedding\n",
    "        # Input:  (batch, seq_len)\n",
    "        # Output: (batch, seq_len, 64)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # STEP 2: Rearranging for Conv1D\n",
    "        # Conv1D expects: (batch, channels, seq_len), So we permute dimensions.\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # STEP 3: Convolutional feature extraction\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # STEP 4: Global pooling\n",
    "        # (batch, 32, seq_len) → (batch, 32, 1)\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # Remove last dimension → (batch, 32)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        # STEP 5: Classification\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "print(\"✓ SimpleProteinCNN class created\")\n",
    "\n",
    "\n",
    "# PARAMETER COUNT\n",
    "# Counting parameters helps us understand:\n",
    "# - Model complexity, Risk of overfitting, Memory requirements\n",
    "\n",
    "sample_model = SimpleProteinCNN(num_classes=len(train_dataset.label_cols))\n",
    "total_params = sum(p.numel() for p in sample_model.parameters())\n",
    "\n",
    "print(f\"  Model parameters: {total_params:,}\")\n",
    "print_memory_usage(\"After creating model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd124d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Setting up device...\n",
      "✓ Using device: cpu\n",
      "After device setup - Memory Usage: 0.02 GB\n",
      "\n",
      "Step 2: Verifying dataset...\n",
      "Training proteins: 71925\n",
      "Validation proteins: 15412\n",
      "Test proteins: 15413\n",
      "✓ Found 2761 GO term labels\n",
      "\n",
      "Step 3: Creating DataLoaders...\n",
      "✓ DataLoaders created\n",
      "  Batch size: 8\n",
      "  Training batches: 8991\n",
      "\n",
      "Step 4: Creating model...\n",
      "✓ Model created: LightweightProteinCNN\n",
      "  Total parameters: 186,345\n",
      "\n",
      "Step 5: Setting up loss and optimizer...\n",
      "✓ Loss function and optimizer initialized\n",
      "\n",
      "Step 6: Running sanity check...\n",
      "Batch shapes:\n",
      "  Sequences: torch.Size([256])\n",
      "  Labels: torch.Size([256, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Model outputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Loss computation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mLightweightProteinCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    107\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Step 2: Rearrange for Conv1D\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# (batch, seq_len, 32) → (batch, 32, seq_len)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Step 3: Convolution + Activation\u001b[39;00m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv(x))\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# DEVICE SETUP\n",
    "# In this section, we define the computational device.\n",
    "# Since we are working on a laptop without GPU configuration, we explicitly select CPU to avoid CUDA-related issues.\n",
    "\n",
    "print(\"Step 1: Setting up device...\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"✓ Using device: {device}\")\n",
    "print_memory_usage(\"After device setup\")\n",
    "\n",
    "# VERIFY DATASET STRUCTURE\n",
    "# Before training, we confirm: The dataset size, The number of GO term labels, That label columns exist\n",
    "# This ensures that our dataset class was constructed correctly.\n",
    "\n",
    "print(\"\\nStep 2: Verifying dataset...\")\n",
    "print(f\"Training proteins: {len(train_dataset)}\")\n",
    "print(f\"Validation proteins: {len(val_dataset)}\")\n",
    "print(f\"Test proteins: {len(test_dataset)}\")\n",
    "\n",
    "if hasattr(train_dataset, \"label_cols\"):\n",
    "    num_classes = len(train_dataset.label_cols)\n",
    "    print(f\"✓ Found {num_classes} GO term labels\")\n",
    "else:\n",
    "    raise ValueError(\"Dataset does not contain label columns.\")\n",
    "\n",
    "\n",
    "# CREATE DATA LOADERS\n",
    "# DataLoader:\n",
    "# - Loads data in batches, Handles shuffling, Improves memory efficiency\n",
    "# num_workers=0 is required for Windows to avoid multiprocessing issues.\n",
    "\n",
    "print(\"\\nStep 3: Creating DataLoaders...\")\n",
    "\n",
    "batch_size = 8  # Small batch size for CPU memory safety\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoaders created\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "\n",
    "\n",
    "# DEFINE MODEL ARCHITECTURE\n",
    "# We define a lightweight CNN suitable for CPU training.\n",
    "# This version is intentionally simple to: Reduce memory usage, Reduce computation time, Ensure stability on laptops\n",
    "\n",
    "print(\"\\nStep 4: Creating model...\")\n",
    "\n",
    "class LightweightProteinCNN(nn.Module):\n",
    "    \n",
    "   # Simplified CNN for multi-label protein function prediction.\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Layer\n",
    "        # Converts amino acid indices into 32-dimensional vectors.\n",
    "        # 21 tokens = 20 amino acids + padding.\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=21,\n",
    "            embedding_dim=32,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # Convolution Layer\n",
    "        # Detects short sequence motifs (kernel_size=3).\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        # Global Max Pooling\n",
    "        # Reduces sequence dimension to 1.\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Output Layer\n",
    "        # One neuron per GO term.\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Step 1: Embedding\n",
    "        # (batch, seq_len) → (batch, seq_len, 32)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Step 2: Rearrange for Conv1D\n",
    "        # (batch, seq_len, 32) → (batch, 32, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Step 3: Convolution + Activation\n",
    "        x = self.relu(self.conv(x))\n",
    "\n",
    "        # Step 4: Global Pooling\n",
    "        # (batch, 64, seq_len) → (batch, 64, 1)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Remove last dimension\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        # Step 5: Output layer (logits)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = LightweightProteinCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "print(f\"✓ Model created: {model.__class__.__name__}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "# DEFINE LOSS FUNCTION AND OPTIMIZER\n",
    "# Since this is multi-label classification, we use BCEWithLogitsLoss (NOT CrossEntropyLoss).\n",
    "#\n",
    "# BCEWithLogitsLoss: Applies sigmoid internally, Computes binary cross-entropy for each label independently\n",
    "\n",
    "print(\"\\nStep 5: Setting up loss and optimizer...\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "print(\"✓ Loss function and optimizer initialized\")\n",
    "\n",
    "\n",
    "# QUICK SANITY CHECK\n",
    "# Before real training, we verify:\n",
    "# 1. DataLoader returns correct shapes\n",
    "# 2. Model forward pass works\n",
    "# 3. Loss computation works\n",
    "# 4. Backpropagation works\n",
    "\n",
    "print(\"\\nStep 6: Running sanity check...\")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "sequences = batch[\"sequence\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  Sequences: {sequences.shape}\")\n",
    "print(f\"  Labels: {labels.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(sequences)\n",
    "print(f\"  Model outputs: {outputs.shape}\")\n",
    "\n",
    "# Loss computation\n",
    "loss = criterion(outputs, labels)\n",
    "print(f\"  Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backpropagation test\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✓ Forward and backward pass successful\")\n",
    "print_memory_usage(\"After training setup\")\n",
    "\n",
    "\n",
    "# FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TRAINING SETUP SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Parameters: {total_params:,}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print(\"\\nREADY FOR TRAINING\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
